# Technical Behavioral Interview Questions & Answers

## Performance & Optimization

**Q: Tell me about a time when you identified and resolved a performance bottleneck.**

Our sensor ingestion API was experiencing severe latency spikes during peak hours, with response times jumping from 200ms to 3+ seconds, causing customer complaints and SLA violations. I instrumented the application with OpenTelemetry and Jaeger for distributed tracing, which revealed that 80% of request time was spent in sequential database queries - specifically an N+1 query problem where we were making individual SELECT queries for each sensor instead of batch fetching. I implemented query optimization using SQLAlchemy's joinedload and added Redis caching with a 5-minute TTL for frequently accessed sensor data. Additionally, I configured Nginx with proxy caching at the edge layer to serve repeated requests without hitting the application. The combination of query batching, Redis caching, and Nginx edge caching reduced P95 latency from 3.2s to 180ms (94% improvement) and increased throughput from 50 req/s to 1000+ req/s.

**Q: Describe a situation where you had to optimize system performance under high load.**

During a product launch, our API started failing under 500+ concurrent requests with database connection pool exhaustion and cascading timeouts across microservices. I implemented a multi-layered approach: first, added Nginx rate limiting with leaky bucket algorithm (20 req/s per IP, burst of 50) to prevent DDoS-style traffic spikes. Second, configured PostgreSQL connection pooling with pgBouncer in transaction mode, increasing effective connections from 100 to 1000 without overwhelming the database. Third, implemented ECS auto-scaling policies based on CPU (>70%) and request count metrics, scaling from 3 to 20 tasks automatically. Fourth, added circuit breakers in the API Gateway using a failure threshold of 5 errors in 60 seconds to prevent cascading failures. Load testing with 1000 concurrent users showed 99.8% success rate with P99 latency under 450ms, compared to previous 60% success rate with frequent timeouts.


**Q: How did you handle a situation where the system couldn't scale to meet demand?**

Our monolithic application was hitting scaling limits at 200 concurrent users, with vertical scaling becoming prohibitively expensive ($5K/month for a single large EC2 instance). I led the migration to a microservices architecture, decomposing the monolith into four services: Auth Service, Sensor Service, Kafka Service, and API Gateway. Each service was containerized with Docker and deployed on ECS Fargate with independent auto-scaling policies. The Auth Service scaled based on authentication requests (1-5 tasks), Sensor Service scaled aggressively based on ingestion load (1-20 tasks), and Kafka Service maintained steady state (2-3 tasks). I implemented Nginx as an L7 load balancer with least-connections algorithm distributing traffic across service instances. This architecture enabled horizontal scaling to 1000+ concurrent users while reducing costs by 60% ($2K/month) through granular resource allocation and auto-scaling that spun down unused capacity during off-peak hours.

## Debugging & Troubleshooting

**Q: Tell me about a complex bug you debugged in a distributed system.**

Users reported intermittent 500 errors that we couldn't reproduce in testing, occurring randomly about 5% of the time during peak hours. I implemented OpenTelemetry distributed tracing across all four microservices with Jaeger visualization, adding correlation IDs to track requests end-to-end. The traces revealed that errors only occurred when the Redis cache was cold (cache miss), causing the Sensor Service to query PostgreSQL, which would timeout after 30 seconds under high load. The issue was a database connection leak in SQLAlchemy where connections weren't being properly returned to the pool after exceptions. I fixed this by implementing proper connection lifecycle management with context managers, added connection pool monitoring with CloudWatch metrics, and implemented a circuit breaker pattern that failed fast after 3 consecutive database timeouts instead of waiting 30 seconds each time. I also added Redis cache warming during deployment to prevent cold cache scenarios. Post-fix, error rate dropped from 5% to 0.02%, and mean time to detection improved from 30 minutes to 2 minutes through distributed tracing.


**Q: Describe a time when you had to troubleshoot a production incident.**

At 2 AM, our monitoring alerted that API response times spiked to 10+ seconds and error rates hit 40%. I immediately checked CloudWatch metrics and saw database CPU at 98% with 500+ active connections. Using Jaeger traces, I identified a recently deployed feature that was executing an unoptimized query with a full table scan on 10M rows without proper indexing. I rolled back the deployment within 5 minutes using our blue-green deployment strategy in ECS, which immediately restored service. Post-incident, I created a composite index on the frequently queried columns (sensor_id, timestamp), reducing query time from 8 seconds to 15ms. I also implemented query performance testing in CI/CD using EXPLAIN ANALYZE, added database query monitoring with pg_stat_statements, and established a policy requiring database review for any queries touching tables with >1M rows. Additionally, I configured CloudWatch alarms for database CPU >80% and slow query logs for queries >1s, reducing MTTR from 15 minutes to under 3 minutes for similar incidents.

## Architecture & Design

**Q: Tell me about a time you had to make a critical architectural decision.**

Our team was debating whether to continue with our monolithic architecture or migrate to microservices as we scaled from 10 to 30 developers across 3 teams. I conducted a technical analysis comparing both approaches: the monolith was simpler operationally but created deployment bottlenecks (30-minute deploys, all-or-nothing releases) and prevented independent team velocity. I proposed a microservices architecture with clear service boundaries based on domain-driven design: Auth (user management, JWT), Sensors (data ingestion, validation), Kafka (event streaming), and API Gateway (routing, aggregation). I built a POC demonstrating independent deployment, technology flexibility (Python for data processing, potential Go for high-performance auth), and fault isolation through circuit breakers. The POC showed we could deploy the Sensor Service 10 times/day without affecting Auth Service, and auto-scale services independently based on their specific load patterns. I also addressed concerns about operational complexity by implementing comprehensive monitoring with CloudWatch, distributed tracing with OpenTelemetry/Jaeger, and infrastructure-as-code with Terraform. The migration took 3 months but resulted in 3x faster feature delivery, 60% cost reduction through granular scaling, and 99.9% uptime through fault isolation.


**Q: How did you approach designing a system for high availability?**

We needed to achieve 99.9% uptime SLA for our sensor data platform serving IoT devices that couldn't tolerate downtime. I designed a multi-layered resilience strategy: implemented circuit breakers in the API Gateway with 5-failure threshold and 60-second timeout to prevent cascading failures between microservices. Configured PostgreSQL with Multi-AZ deployment and read replicas for failover, with automatic promotion in under 60 seconds. Added Nginx health checks with max_fails=3 and fail_timeout=30s to automatically route traffic away from unhealthy instances. Implemented event-driven architecture with Kafka for asynchronous processing, ensuring sensor data was never lost even during service outages through persistent message queues and dead letter queues for failed processing. Configured ECS auto-scaling with target tracking policies maintaining 70% CPU utilization, allowing automatic scale-up during traffic spikes. Added comprehensive monitoring with CloudWatch alarms for CPU >80%, memory >85%, error rate >1%, and latency >500ms, with PagerDuty integration for 24/7 on-call. Implemented blue-green deployments with health check validation before traffic cutover, enabling zero-downtime releases. This architecture achieved 99.94% uptime over 6 months with MTTR under 5 minutes for incidents.

## Security & Compliance

**Q: Describe a time when you identified and fixed a security vulnerability.**

During a security audit, we discovered our API was vulnerable to DDoS attacks and had no rate limiting, allowing malicious actors to overwhelm our services with unlimited requests. I implemented a comprehensive rate limiting strategy using Nginx with multiple zones: strict rate limiting for auth endpoints (3 req/s per IP with burst of 5) to prevent brute force attacks, moderate limiting for write operations (5 req/s with burst of 10) to prevent data manipulation, and generous limiting for read operations (50 req/s with burst of 100) for legitimate high-volume clients. I also added connection limiting (10 concurrent connections per IP) to prevent connection exhaustion attacks. At the application layer, I implemented JWT token authentication with RS256 asymmetric encryption, 15-minute access token expiry, and refresh token rotation to prevent token theft. Added security headers (X-Frame-Options, X-Content-Type-Options, X-XSS-Protection) through Nginx to prevent clickjacking and XSS attacks. Implemented API key rotation policy requiring keys to be rotated every 90 days, with automatic expiration and CloudWatch alerts. Configured AWS WAF rules blocking common attack patterns (SQL injection, XSS) and geo-blocking from high-risk countries. Post-implementation, we successfully mitigated 3 DDoS attempts averaging 10K req/s, with rate limiting automatically throttling malicious traffic while maintaining service for legitimate users.


## DevOps & Infrastructure

**Q: Tell me about a time you improved the deployment process.**

Our deployment process was manual, error-prone, and took 45 minutes with frequent rollbacks due to configuration drift between environments. I implemented a complete CI/CD pipeline using GitHub Actions with infrastructure-as-code using Terraform. The pipeline had three stages: CI stage running pytest unit tests, pylint code quality checks, and bandit security scanning on every commit. Build stage creating Docker images with multi-stage builds (reducing image size from 1.2GB to 380MB) and pushing to ECR with semantic versioning tags. Deploy stage using Terraform to update ECS task definitions with blue-green deployment strategy, where new tasks were deployed alongside old ones, health checks validated the new version for 5 minutes, and traffic was gradually shifted using ALB weighted target groups (10%, 50%, 100% over 15 minutes). I added automatic rollback triggers if error rate exceeded 1% or P95 latency exceeded 500ms during deployment. Implemented environment parity using Terraform workspaces (dev, staging, prod) with identical infrastructure configurations but different scaling parameters. Added deployment notifications to Slack with deployment status, version numbers, and rollback commands. This reduced deployment time from 45 minutes to 8 minutes, decreased deployment failures from 15% to under 2%, and enabled 10+ deployments per day compared to previous 2-3 per week.

**Q: How did you handle infrastructure cost optimization?**

Our AWS bill was $8K/month and growing 20% monthly, with management requesting 40% cost reduction without impacting performance. I conducted a comprehensive cost analysis using AWS Cost Explorer and identified three major areas: over-provisioned ECS tasks running 24/7 at low utilization (40% of costs), expensive RDS instance running continuously (25% of costs), and unoptimized S3 storage with no lifecycle policies (15% of costs). I implemented aggressive auto-scaling policies for ECS, scaling down to 1 task during off-peak hours (midnight-6am) and scaling up to 20 tasks during peak (9am-5pm), reducing average task count from 15 to 8. Configured RDS with Aurora Serverless v2 that automatically scaled capacity based on load, reducing costs by 50% during low-traffic periods. Implemented S3 lifecycle policies moving sensor data older than 30 days to S3 Glacier (90% cheaper) and deleting data older than 1 year per retention policy. Added CloudWatch metrics for cost tracking per service, enabling granular cost attribution. Implemented Reserved Instances for baseline capacity (3 ECS tasks, 1 RDS instance) with 1-year commitment for 40% discount. Optimized Docker images removing unnecessary dependencies and using Alpine Linux base images, reducing deployment times and ECR storage costs. These optimizations reduced monthly costs from $8K to $4.2K (47% reduction) while maintaining 99.9% uptime and improving P95 latency from 320ms to 280ms through more efficient resource utilization.


## Data & Scalability

**Q: Describe how you handled a data pipeline that was failing under load.**

Our sensor data pipeline was dropping 20% of incoming events during peak hours (1000+ events/second), causing data loss and customer complaints. Using CloudWatch metrics and distributed tracing, I identified that the synchronous processing model was the bottleneck - the API was waiting for database writes and S3 archival to complete before responding, causing request timeouts under load. I redesigned the pipeline to be event-driven using Kafka as a message broker: sensor data was immediately published to Kafka topics (5ms operation) and the API responded with 202 Accepted, while separate consumer services processed data asynchronously. Implemented three consumer groups: one for real-time PostgreSQL writes with batch inserts (100 records per transaction), one for S3 archival with Parquet compression (reducing storage by 70%), and one for real-time analytics. Added Kafka partitioning by sensor_id for parallel processing across 10 partitions, enabling horizontal scaling of consumers. Implemented dead letter queues for failed messages with exponential backoff retry (1s, 2s, 4s, 8s, 16s) and manual review after 5 failures. Added idempotency keys to prevent duplicate processing during retries. This architecture handled 5000+ events/second with zero data loss, reduced API latency from 800ms to 50ms, and enabled replay capability for data recovery scenarios.

**Q: Tell me about a time you optimized database performance.**

Our PostgreSQL database was experiencing severe performance degradation with queries taking 5-10 seconds during peak hours, causing API timeouts and poor user experience. I used pg_stat_statements to identify slow queries and found several issues: missing indexes on frequently queried columns (sensor_id, timestamp), inefficient JOIN operations scanning millions of rows, and lack of query result caching. I created composite indexes on (sensor_id, timestamp DESC) for time-series queries, reducing query time from 8s to 120ms. Implemented PostgreSQL query plan analysis using EXPLAIN ANALYZE to identify sequential scans and optimized them with proper indexes. Added database connection pooling with pgBouncer in transaction mode, increasing effective connections from 100 to 1000 without overwhelming the database. Configured PostgreSQL autovacuum more aggressively (scale_factor=0.05) to prevent table bloat. Implemented read replicas for analytics queries, routing read-heavy operations to replicas and keeping the primary for writes only. Added Redis caching layer with 5-minute TTL for frequently accessed sensor data, reducing database load by 60%. Implemented query result caching at the application layer using Redis with cache invalidation on writes. Optimized slow aggregation queries by creating materialized views refreshed every 5 minutes. These optimizations reduced P95 query time from 5.2s to 85ms, increased throughput from 200 to 2000 queries/second, and reduced database CPU from 95% to 45%.


## Monitoring & Observability

**Q: How did you improve system observability and monitoring?**

Our team was spending hours debugging production issues because we lacked visibility into system behavior, with mean time to detection (MTTD) at 30 minutes and mean time to resolution (MTTR) at 2 hours. I implemented a comprehensive observability stack: deployed OpenTelemetry instrumentation across all microservices for distributed tracing, automatically capturing HTTP requests, database queries, cache operations, and external API calls. Integrated Jaeger for trace visualization, enabling engineers to see complete request flows across services and identify bottlenecks in seconds rather than hours. Configured CloudWatch with custom metrics for business-critical KPIs: request rate, error rate, P50/P95/P99 latency, database connection pool utilization, cache hit rates, and Kafka consumer lag. Created CloudWatch dashboards with real-time visualization of system health, service dependencies, and performance trends. Implemented structured logging with JSON format including correlation IDs, trace IDs, user IDs, and request metadata, enabling log aggregation and analysis. Set up CloudWatch alarms with tiered severity: P1 for customer-impacting issues (error rate >5%, latency >1s), P2 for degraded performance (CPU >80%, memory >85%), P3 for capacity warnings (disk >70%). Integrated PagerDuty for on-call rotation with escalation policies. Added synthetic monitoring using CloudWatch Synthetics to proactively detect issues before customers. This reduced MTTD from 30 minutes to 2 minutes (93% improvement) and MTTR from 2 hours to 15 minutes (87% improvement), while enabling data-driven performance optimization that improved P95 latency by 40%.

**Q: Describe a time when monitoring helped you prevent a major incident.**

Our CloudWatch alarms detected database connection pool utilization at 85% and rising, with projections showing exhaustion within 30 minutes during peak traffic. I immediately investigated using distributed tracing and found a recently deployed feature was leaking database connections by not properly closing them in exception handlers. I quickly deployed a hotfix implementing proper connection lifecycle management with context managers and try-finally blocks. However, the connection pool was still at 80% due to existing leaked connections. I implemented an emergency mitigation: increased the connection pool size from 100 to 200 temporarily, added pgBouncer in transaction mode to multiplex connections more efficiently, and implemented connection pool monitoring with automatic alerts. I also added circuit breakers to prevent cascading failures if the database became unavailable. Post-incident, I implemented connection pool metrics in CloudWatch with alarms at 70% utilization, added automated connection leak detection in CI/CD using connection pool monitoring during integration tests, and established a policy requiring database connection review for all code changes. The proactive monitoring prevented what would have been a complete service outage affecting all customers, and the systematic fixes reduced connection pool utilization from 80% to 35% under normal load.


## Innovation & Automation

**Q: Tell me about a time you introduced a new technology or tool that improved productivity.**

Our development team was spending 30% of their time writing boilerplate API code and manually testing endpoints, slowing feature delivery. I researched and proposed implementing LangChain with GPT-4 for natural language API routing, allowing developers and QA to interact with APIs using plain English instead of remembering exact endpoint syntax. I built a POC demonstrating that users could query "show me all temperature sensors" and the system would automatically route to GET /api/sensors?type=temperature with 95% accuracy. The implementation used LangChain for intent recognition, entity extraction, and API mapping, with custom prompts trained on our API documentation. I integrated this into our API Gateway as an optional /api/nl endpoint, maintaining backward compatibility with existing REST endpoints. Added comprehensive logging of natural language queries and their translations for continuous improvement. The system reduced API integration time for new developers from 2 weeks to 3 days, decreased QA testing time by 40% through natural language test case execution, and improved developer productivity by 20-30% by eliminating the need to reference API documentation constantly. I also created interactive demos for customer-facing teams, enabling non-technical staff to query production data without SQL knowledge.

**Q: How did you automate a manual process that was causing problems?**

Our infrastructure provisioning was completely manual, taking 4 hours per environment with frequent configuration drift causing production incidents. I implemented infrastructure-as-code using Terraform, codifying all AWS resources: VPC with public/private subnets, ECS clusters with auto-scaling, RDS with Multi-AZ, ALB with target groups, CloudWatch alarms, and IAM roles. Created Terraform modules for reusable components (networking, compute, database) with parameterized inputs for environment-specific configurations. Implemented Terraform workspaces for environment isolation (dev, staging, prod) with identical infrastructure but different scaling parameters. Added Terraform state management using S3 backend with DynamoDB locking to prevent concurrent modifications. Integrated Terraform into CI/CD pipeline with automated plan on pull requests and apply on merge to main branch. Implemented drift detection running daily Terraform plan to identify manual changes and alert the team. Added pre-commit hooks validating Terraform syntax and running security scans with tfsec. Created comprehensive documentation with architecture diagrams generated from Terraform code using terraform-docs. This reduced environment provisioning from 4 hours to 15 minutes, eliminated configuration drift incidents (previously 2-3 per month), enabled disaster recovery with infrastructure recreation in under 30 minutes, and improved team velocity by allowing developers to spin up isolated test environments on-demand.


## Collaboration & Leadership

**Q: Describe a time when you had to convince your team to adopt a new approach.**

Our team was resistant to implementing distributed tracing because they perceived it as adding complexity and overhead without clear benefits. I built a compelling case by first implementing a POC with OpenTelemetry and Jaeger on a single service, demonstrating that I could identify a slow database query (taking 800ms) in under 2 minutes using trace visualization, compared to the 2 hours it took us to debug a similar issue the previous week using log analysis. I presented metrics showing the instrumentation added only 2-3ms overhead per request (0.5% impact) while providing complete visibility into request flows across services. I created an interactive demo showing how traces revealed the exact sequence of operations, timing of each span, and relationships between services, making it immediately obvious where bottlenecks existed. I addressed concerns about operational complexity by demonstrating Jaeger's simple Docker deployment and showing how automatic instrumentation required minimal code changes (just adding middleware). I also highlighted how distributed tracing would reduce our MTTR from 2 hours to under 15 minutes, saving approximately 40 engineering hours per month in debugging time. After the demo, the team was convinced and we rolled out tracing across all services. Within the first month, we identified and fixed 3 major performance issues that had been plaguing us for months, validating the investment and making the team advocates for observability.

**Q: Tell me about a time you mentored someone or shared knowledge with your team.**

A junior developer on our team was struggling with understanding microservices architecture and kept making design decisions that created tight coupling between services. I created a comprehensive architecture decision framework documenting our patterns: microservices architecture for service decomposition, API Gateway pattern for centralized routing, event-driven architecture for asynchronous communication, circuit breaker pattern for resilience, and repository pattern for data access. I organized weekly architecture review sessions where we analyzed real-world scenarios and discussed tradeoffs between different approaches. I built interactive tools like the architecture decision framework Python script that asked questions about team size, scaling needs, and domain clarity to recommend appropriate patterns. I created runnable demos for each pattern with detailed documentation explaining the what, why, and how. For the microservices migration, I paired with the junior developer on decomposing a monolithic module into a microservice, teaching service boundary identification, API design, database per service pattern, and inter-service communication. I established code review guidelines focusing on architectural principles and created a knowledge base with common patterns and anti-patterns. Within 3 months, the junior developer was confidently designing new microservices and even identified an opportunity to extract a shared authentication library that reduced code duplication across services by 40%. The knowledge sharing also improved team velocity by 25% as developers could make architectural decisions independently without constant guidance.


## Problem Solving & Critical Thinking

**Q: Tell me about a time you had to make a decision with incomplete information.**

During a critical production incident at 3 AM, our API was returning 500 errors for 30% of requests, but our monitoring showed all services healthy and database metrics normal. With limited information and customers impacted, I had to make quick decisions. I immediately implemented a rollback to the previous version using our blue-green deployment, which took 3 minutes and restored service to 99% success rate. However, 1% of requests were still failing, indicating a deeper issue. With incomplete data, I made the decision to enable verbose logging temporarily (accepting the performance impact) and deployed a hotfix adding detailed error tracking. The logs revealed that the issue was intermittent Redis connection timeouts occurring only when cache keys exceeded 1MB due to a recent feature storing large JSON payloads. I implemented an immediate mitigation by adding Redis connection retry logic with exponential backoff and reducing cache TTL from 1 hour to 5 minutes to prevent large payload accumulation. For the long-term fix, I implemented payload compression using gzip before caching, reducing average payload size by 80%, and added Redis connection pool monitoring with CloudWatch alarms. I also implemented cache key size limits (100KB) with automatic fallback to database queries for oversized data. The decision to rollback first (restoring service) then investigate (finding root cause) rather than debugging in production proved correct, as it minimized customer impact while allowing systematic problem solving.

**Q: Describe a situation where you had to balance technical debt with feature delivery.**

Product management was pushing for rapid feature delivery (5 new features in 2 months) while our technical debt was mounting: no automated testing (manual QA taking 2 days per release), monolithic architecture preventing independent deployments, and no monitoring causing long incident resolution times. I proposed a balanced approach: dedicate 60% of sprint capacity to features and 40% to technical debt reduction, with specific measurable goals. For testing, I implemented pytest with 50+ unit tests and integration tests, achieving 85% code coverage and reducing QA time from 2 days to 4 hours. For architecture, I extracted the most frequently changing component (sensor ingestion) into a microservice, enabling independent deployment and reducing deployment time from 45 minutes to 8 minutes. For monitoring, I implemented CloudWatch metrics and alarms, reducing MTTR from 2 hours to 15 minutes. I demonstrated ROI by showing that the 40% technical debt investment actually increased feature delivery velocity by 35% within 2 months due to faster testing, deployment, and debugging. I created a technical debt backlog with priority scoring based on impact (customer-facing vs internal) and effort (days vs weeks), ensuring we tackled high-impact, low-effort items first. This approach satisfied product management's feature needs while systematically reducing technical debt, and the improved velocity convinced leadership to maintain the 60/40 split long-term.


## Resilience & Reliability

**Q: How did you design a system to handle failures gracefully?**

We needed to ensure our sensor data platform remained operational even when individual components failed, as IoT devices couldn't tolerate data loss. I implemented a comprehensive resilience strategy using multiple patterns: circuit breakers in the API Gateway with 5-failure threshold and 60-second timeout preventing cascading failures when downstream services were unhealthy. Implemented retry logic with exponential backoff (1s, 2s, 4s, 8s, 16s) and jitter to prevent thundering herd problems. Added bulkhead pattern isolating thread pools per service (auth: 10 threads, sensors: 50 threads, kafka: 20 threads) so one slow service couldn't exhaust all resources. Configured health checks at multiple levels: Nginx checking backend health every 10 seconds with max_fails=3, ECS health checks validating container health, and ALB target group health checks ensuring traffic only routed to healthy instances. Implemented graceful degradation where non-critical features (analytics, reporting) could fail without impacting core functionality (sensor ingestion). Added fallback mechanisms: if Redis cache failed, fall back to database; if primary database failed, fall back to read replica; if Kafka failed, buffer events in memory with disk persistence. Implemented idempotency keys preventing duplicate processing during retries. Added dead letter queues for failed messages with manual review process. This architecture survived multiple failure scenarios during chaos engineering tests: database failover (60-second downtime, zero data loss), Redis failure (10% latency increase, no errors), and Kafka outage (events buffered and replayed after recovery).

**Q: Tell me about a time you implemented disaster recovery procedures.**

Our platform had no disaster recovery plan, creating significant business risk if AWS region failed or data was corrupted. I designed and implemented a comprehensive DR strategy: configured PostgreSQL with automated daily backups retained for 30 days, with point-in-time recovery enabling restoration to any second within the retention period. Implemented cross-region replication for S3 sensor data archives to a secondary region with versioning enabled to protect against accidental deletion. Created Terraform configurations for complete infrastructure recreation in a secondary region, tested quarterly with full DR drills. Implemented database backup verification by automatically restoring backups to a test environment and running validation queries. Added Kafka topic replication with MirrorMaker 2.0 for cross-region event streaming. Documented runbooks with step-by-step recovery procedures including RTO (Recovery Time Objective: 4 hours) and RPO (Recovery Point Objective: 5 minutes). Implemented automated backup monitoring with CloudWatch alarms alerting if backups failed or were older than 25 hours. Created disaster recovery dashboard showing backup status, replication lag, and last successful DR drill. Conducted quarterly DR drills where we simulated region failure and measured actual recovery time (achieved 2.5 hours vs 4-hour target). Added automated testing of backup restoration in CI/CD pipeline. This DR strategy gave business confidence to sign enterprise contracts requiring 99.9% uptime SLA and <1 hour RTO, and successfully recovered from a production database corruption incident with only 3 minutes of data loss.


## API Design & Integration

**Q: Describe how you designed and implemented a scalable API.**

We needed to build an API serving 1000+ concurrent IoT devices with strict latency requirements (<200ms P95) and high availability. I designed a RESTful API using FastAPI with async/await for non-blocking I/O, enabling a single instance to handle 1000+ concurrent connections. Implemented API versioning (v1, v2) in the URL path allowing backward compatibility while evolving the API. Added comprehensive input validation using Pydantic models with type checking, range validation, and custom validators preventing invalid data from entering the system. Implemented pagination for list endpoints using cursor-based pagination (more efficient than offset-based for large datasets) with default page size of 100 and maximum of 1000. Added filtering, sorting, and field selection query parameters enabling clients to request only needed data, reducing payload sizes by 60%. Implemented rate limiting with different tiers: 10 req/s for general endpoints, 50 req/s for read-heavy endpoints, 3 req/s for auth endpoints, preventing abuse while allowing legitimate high-volume usage. Added comprehensive API documentation using OpenAPI/Swagger with interactive testing, request/response examples, and error code documentation. Implemented proper HTTP status codes (200 for success, 201 for creation, 400 for validation errors, 401 for auth failures, 429 for rate limiting, 500 for server errors) with detailed error messages including error codes and suggested fixes. Added CORS configuration allowing cross-origin requests from approved domains. Implemented request/response compression using gzip reducing bandwidth by 70%. Added ETag headers for caching and conditional requests. This API design handled 5000+ req/s with P95 latency of 180ms and 99.9% uptime.

**Q: Tell me about a time you integrated with a third-party API that had reliability issues.**

We needed to integrate with an external weather API for sensor data enrichment, but the API had frequent outages (99.5% uptime) and variable latency (200ms-5s). I implemented a resilient integration strategy: added circuit breaker pattern with 5-failure threshold opening the circuit for 60 seconds, preventing our system from waiting on a failing service. Implemented aggressive timeouts (2-second connection timeout, 5-second read timeout) failing fast rather than blocking threads. Added retry logic with exponential backoff (1s, 2s, 4s) but only for idempotent operations and specific error codes (500, 502, 503, 504). Implemented response caching with Redis storing weather data for 1 hour (weather doesn't change frequently), reducing API calls by 80% and providing fallback data during outages. Added fallback to stale cache data (up to 6 hours old) when the API was unavailable, preferring slightly outdated data over no data. Implemented request queuing with rate limiting to respect the third-party API's rate limits (100 req/min), preventing our integration from being blocked. Added comprehensive monitoring tracking API availability, latency, error rates, and cache hit rates with CloudWatch alarms. Implemented graceful degradation where sensor data was still processed without weather enrichment if the API was unavailable. Added detailed logging of all API interactions for debugging and SLA tracking. This integration achieved 99.95% effective availability (higher than the third-party API's 99.5%) through caching and fallbacks, with P95 latency of 50ms (vs 200ms-5s direct API calls).


## Testing & Quality

**Q: How did you improve testing practices in your team?**

Our codebase had zero automated tests, relying entirely on manual QA that took 2 days per release and frequently missed bugs that reached production. I implemented a comprehensive testing strategy starting with unit tests using pytest, achieving 85% code coverage within 2 months by making testing part of the definition of done for every feature. Created integration tests validating API endpoints end-to-end, including database interactions and external service mocks using pytest-mock. Implemented load testing with locust simulating 1000 concurrent users, identifying performance bottlenecks before production deployment. Added contract testing between microservices using Pact, ensuring API changes didn't break dependent services. Implemented database migration testing validating that schema changes could be applied and rolled back without data loss. Added security testing with bandit scanning for common vulnerabilities (SQL injection, XSS, hardcoded secrets) in CI/CD pipeline. Created test fixtures and factories using pytest-factory-boy for consistent test data generation. Implemented test parallelization reducing test suite execution from 15 minutes to 3 minutes. Added code coverage reporting with coverage.py and configured CI/CD to fail if coverage dropped below 80%. Created testing documentation with examples and best practices. Implemented pre-commit hooks running tests locally before push. Added mutation testing with mutmut validating test quality by introducing bugs and ensuring tests caught them. This testing infrastructure reduced production bugs by 70%, decreased QA time from 2 days to 4 hours, and increased deployment confidence enabling 10+ deployments per day vs previous 2-3 per week.

**Q: Describe a time when you caught a critical bug before it reached production.**

During code review, I noticed a new feature was using string concatenation to build SQL queries instead of parameterized queries, creating a SQL injection vulnerability. I immediately flagged this as a security risk and demonstrated the vulnerability by showing how an attacker could inject malicious SQL through user input. I worked with the developer to refactor the code using SQLAlchemy's parameterized queries with bound parameters, eliminating the injection risk. However, this made me realize we had no systematic way to catch such vulnerabilities. I implemented bandit security scanning in the CI/CD pipeline, which scans Python code for common security issues including SQL injection, hardcoded passwords, insecure random number generation, and unsafe deserialization. Added SAST (Static Application Security Testing) with SonarQube analyzing code quality and security vulnerabilities. Implemented dependency scanning with safety checking for known vulnerabilities in third-party packages, automatically failing builds if critical vulnerabilities were found. Added secrets scanning with git-secrets preventing accidental commit of API keys, passwords, and tokens. Created security testing guidelines requiring security review for any code handling user input, authentication, or database queries. Implemented automated penetration testing in staging environment using OWASP ZAP. Added security training for the team covering OWASP Top 10 vulnerabilities. These security measures caught 12 potential vulnerabilities in the first month, including 3 critical issues that would have been exploitable in production, and established a security-first culture in the team.

